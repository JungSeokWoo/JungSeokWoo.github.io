- layout: top-middle
  name: Perception for autonomous driving & AR navigation
  quote: >
    (2020 @ LGE) Developed state-of-the-art lane detection for autonomous driving & AR Navigation.
  description: | # this will include new lines to allow paragraphs
    <div align="center">
      <img src="/images/ld_highway.JPG" width="80%">
    </div>
    
    I proposed fast and robust lane detection method based on an instance segmentation approach. This method achieved global rank 3 on the Tusimple Lane Detection Benchmark([link](https://paperswithcode.com/sota/lane-detection-on-tusimple)) and real-time performance on mobile platform.
    My paper has been accepted in [ECCV 2020 Workshop on Perception for Autonomous Driving](https://sites.google.com/view/pad2020).
    I implemented GPU/DSP/NPU runtime based DNN application by using <mark>Android</mark>, <mark>C++JNI</mark>, <mark>OpenCL</mark> and Snapdragon Neural Processing Engine SDK(<mark>SNPE</mark>).
    
    * Paper : **Seokwoo Jung**, Sungha Choi, Mohammad A. Khan, Jaegul Choo, "Towards Lightweight Lane Detection by Optimizing Spatial Embedding". arxiv [link](https://arxiv.org/abs/2008.08311)
    * 3-min presentation video : [link](https://www.youtube.com/watch?v=2lrtB-NBw2E)

- layout: top-middle
  name: Autonomous driving in urban
  quote: >
    (2018~2019 @ LGE) Developed perception algorithms by using multiple sensors.
  description: | # this will include new lines to allow paragraphst
    <div align="center">
      <img src="/images/lg_autonomous_vehicle.jpg" width="60%">
    </div>
    
    We developed an autonomous vehicle capable of driving in an urban environment.
    I developed lane detection, freespace detection and occupancy grid mapping <mark>ROS</mark> packages, and used several cameras and lidars by sensor fusion.
    I designed a single DNN network that is jointly trained on independent datasets, and accelerated it using <mark>TensorRT</mark> and <mark>CUDA</mark> on the NVIDIA Platform.
    During this period, I have grown my ability to implement the SOTA deep learning papers in various frameworks (<mark>Caffe</mark>, <mark>TensorFlow</mark>, <mark>Pytorch</mark>).
    
    * Promotional video : [link](https://www.youtube.com/watch?v=Ao_2IXEt6ac)
    * News & Press : [link1](https://biz.chosun.com/site/data/html_dir/2019/10/10/2019101001733.html), [link2](https://www.youtube.com/watch?v=hLkUf_cIHDs)
    
    
- layout: top-middle
  name: EureCar &#58; The self-driving car for urban driving and racing
  quote: >
    (2015~2017 @ KAIST) Developed deep learning based perception system and end-to-end driving 
  description: | # this will include new lines to allow paragraphst
    <div align="center">
      <img src="/images/eurecar.jpg" width="80%">
    </div>
    
    I was a lead perception engieer of 'Eurecar', a representative self-driving car of KAIST.
    I designed a vision system using cameras and lidars.
    We demonstrated an advanced autonomous driving by applying the SOTA deep learning.
    In more detail, I used semantic segmentation and object detection to recognize the driving environment, and convert this information into vehicle coordinates to be used for autonomous driving.
    In addition to research on perception, I also participated in the development of Eurecar's end-to-end self-driving research.
    During these deep learning studies, the application was implemented using the <mark>Caffe</mark> framework on the NVIDIA platform (Gerforce series, <mark>DrivePX2</mark>, <mark>Jetson</mark> Series).
    
    + Papers :
      - **Seokwoo Jung**, "Development of driving environment recognition system for autonomous vehicle using deep learning based image processing", KAIST Master thesis, 2017
      - Unghui Lee, Jiwon Jung, **Seokwoo Jung**, David Hyunchul Shim, "Development of a self-driving car that can handle the adverse weather", IJAT, 2018
      - Chanyoung Jung, **Seokwoo Jung**, David Hyunchul Shim, "A Hybrid Control Architecture For Autonomous Driving In Urban Environment", ICARCV, 2018
      - **Seokwoo Jung**, Unghui Lee, Jiwon Jung, David Hyunchul Shim, "Real-time Traffic Sign Recognition system with deep convolutional neural network", URAI, 2016
    * Awards : 2015 Minister's(Science and ICT) award, 2016 Minister's(Science and ICT) award
    <div align="center">
      <img src="/images/ministers_awards.jpg" width="60%">
    </div>
    + Videos :
      - Autonomous driving in license test course [link1](https://youtu.be/rbIACvhG49M), [link2(GUI application view)](https://youtu.be/ZI6NrQ3ZK2w)
      - Autonomous driving in racing circuit [link](https://youtu.be/AqjSq4jFA-M)
      - Autonomous parking [link](https://youtu.be/K9iBUegDhdo)
      - End-to-End driving [link](https://youtu.be/dq3yNzIHiCc)
    * News & Press : [link1](http://news.kbs.co.kr/news/view.do?ncd=3369300), [link2(Discovery channel canada)](https://ko-kr.facebook.com/KAIST.official/videos/1540370752676543/)
    * I presented our self-driving car research achievements at [NVIDIA Deep Learning Day 2017](https://www.nvidia.com/ko-kr/deep-learning-day/agenda/).
    <div align="center">
      <img src="/images/nvidia_deeplearning_day2017.png" width="60%">
    </div>
    
- layout: top-middle
  name: International Robotic Challenge (MBZIRC) 2017
  link: https://www.mbzirc.com/challenge/2017
  quote: >
    (2017 @ KAIST) Developed self-navigating, deep learning based visual servoing for UGV robot.
  description: | # this will include new lines to allow paragraphst
    <div align="center">
      <img src="/images/mbzirc_KAIST.jpg" width="60%">
    </div>
    
    I was a lead perception engineer of KAIST team on MBZIRC 2017 Challenge 2.
    In MBZIRC 2017, 45 finalists were chosen from over 316 teams from 47 countries. Our team ranked fourth in total score in the challenge 2, and successfully completed all UGV missions in the grand challenge.
    <div align="center">
      <img src="/images/challenge2_score.jpg" width="60%">
    </div>
    
    * Paper : Jun Hong, **Seokwoo Jung**, Chanyoung Jung, Jiwon Jung, David Hyunchul Shim, "A general‐purpose task execution framework for manipulation mission of the 2017 Mohamed Bin Zayed International Robotics Challenge", Journal of Field Robotics, 2019
    * News & Press : [link1(eng)](https://www.businesswire.com/news/home/20161219006059/en/Final-Dates-Set-Major-International-Robotics-Challenge) [link2(kor)](http://www.irobotnews.com/news/articleView.html?idxno=9419)
    * Documentary : KBS 길에서 만난 과학 [4회](http://vod.kbs.co.kr/index.html?source=episode&sname=vod&stype=vod&program_code=T2017-0125&program_id=PS-2017033216-01-000&broadcast_complete_yn=N&local_station_code=60), [5회](http://vod.kbs.co.kr/index.html?source=episode&sname=vod&stype=vod&program_code=T2017-0125&program_id=PS-2017038870-01-000&broadcast_complete_yn=N&local_station_code=60#more), [6회](http://vod.kbs.co.kr/index.html?source=episode&sname=vod&stype=vod&program_code=T2017-0125&program_id=PS-2017038906-01-000&broadcast_complete_yn=N&local_station_code=60), [7회](http://vod.kbs.co.kr/index.html?source=episode&sname=vod&stype=vod&program_code=T2017-0125&program_id=PS-2017042993-01-000&broadcast_complete_yn=N&local_station_code=60)
    * Source code : github repo [link](https://github.com/Jun-Hong-7794/Eurecar-URV)
    
